# Spark Processor - User Events Real-time Streaming

## ğŸ“– Giá»›i Thiá»‡u

Dá»± Ã¡n nÃ y cung cáº¥p má»™t há»‡ thá»‘ng xá»­ lÃ½ streaming real-time sá»­ dá»¥ng **Apache Spark Structured Streaming** Ä‘á»ƒ subscribe topic `user-events` tá»« **Kafka** vÃ  lÆ°u trá»¯ dá»¯ liá»‡u vÃ o **PostgreSQL**.

### TÃ­nh NÄƒng ChÃ­nh
- âœ… **Real-time Processing**: Xá»­ lÃ½ dá»¯ liá»‡u theo thá»i gian thá»±c
- âœ… **Fault Tolerance**: Checkpoint mechanism Ä‘á»ƒ recovery khi lá»—i
- âœ… **Scalable**: CÃ³ thá»ƒ scale horizontal vá»›i Kafka partitions
- âœ… **Production-Ready**: Error handling, logging, monitoring Ä‘áº§y Ä‘á»§
- âœ… **Well-Documented**: Comment chi tiáº¿t tá»«ng bÆ°á»›c báº±ng tiáº¿ng Viá»‡t

---

## ğŸ“ Cáº¥u TrÃºc ThÆ° Má»¥c

```
spark_processor/
â”œâ”€â”€ processor.py                      # â­ Code chÃ­nh - HÃ m subscribe_user_events()
â”œâ”€â”€ run_user_events_subscriber.py     # ğŸš€ Script Ä‘á»ƒ cháº¡y subscriber
â”œâ”€â”€ setup_user_events_table.sql       # ğŸ—„ï¸ SQL script táº¡o database schema
â”œâ”€â”€ README_USER_EVENTS.md             # ğŸ“– Documentation chi tiáº¿t
â”œâ”€â”€ ARCHITECTURE.md                   # ğŸ—ï¸ SÆ¡ Ä‘á»“ kiáº¿n trÃºc há»‡ thá»‘ng
â”œâ”€â”€ SUMMARY.md                        # ğŸ“ TÃ³m táº¯t cÃ´ng viá»‡c
â”œâ”€â”€ QUICK_REFERENCE.md                # ğŸ“‹ Quick reference guide
â”œâ”€â”€ requirements.txt                  # ğŸ“¦ Python dependencies
â”œâ”€â”€ Dockerfile                        # ğŸ³ Docker configuration
â””â”€â”€ README.md                         # ğŸ“„ File nÃ y
```

---

## ğŸš€ Quick Start

### BÆ°á»›c 1: CÃ i Äáº·t Dependencies
```bash
pip install -r requirements.txt
```

### BÆ°á»›c 2: Setup Database
```bash
psql -U postgres -d your_database -f setup_user_events_table.sql
```

### BÆ°á»›c 3: Set Environment Variables
```bash
export KAFKA_BROKER=localhost:9092
export POSTGRES_USER=postgres
export POSTGRES_PASSWORD=your_password
export POSTGRES_HOST=localhost
export POSTGRES_DB=your_database
```

### BÆ°á»›c 4: Cháº¡y Subscriber
```bash
python run_user_events_subscriber.py
```

---

## ğŸ“š Documentation

### 1. [README_USER_EVENTS.md](README_USER_EVENTS.md)
**HÆ°á»›ng dáº«n chi tiáº¿t** vá»:
- CÃ¡ch sá»­ dá»¥ng hÃ m `subscribe_user_events()`
- Giáº£i thÃ­ch tá»«ng bÆ°á»›c xá»­ lÃ½
- Monitoring & debugging
- Troubleshooting
- Performance tips

### 2. [ARCHITECTURE.md](ARCHITECTURE.md)
**SÆ¡ Ä‘á»“ kiáº¿n trÃºc** bao gá»“m:
- Data flow diagram
- Processing timeline
- Fault tolerance mechanism
- Scaling strategy
- Monitoring points

### 3. [SUMMARY.md](SUMMARY.md)
**TÃ³m táº¯t cÃ´ng viá»‡c** Ä‘Ã£ hoÃ n thÃ nh:
- CÃ¡c tÃ­nh nÄƒng Ä‘Ã£ implement
- Cáº¥u trÃºc code
- Káº¿t quáº£ Ä‘áº¡t Ä‘Æ°á»£c

### 4. [QUICK_REFERENCE.md](QUICK_REFERENCE.md)
**Quick reference** cho:
- Common commands
- SQL queries
- Debugging tips
- Configuration options

---

## ğŸ¯ HÃ m ChÃ­nh: `subscribe_user_events()`

### MÃ´ Táº£
HÃ m nÃ y subscribe topic `user-events` tá»« Kafka vÃ  lÆ°u **Táº¤T Cáº¢** message vÃ o PostgreSQL real-time.

### CÃ¡c BÆ°á»›c Xá»­ LÃ½

#### 1ï¸âƒ£ Khá»Ÿi táº¡o Spark Session
```python
spark = SparkSession.builder.appName("UserEventsSubscriber").getOrCreate()
```

#### 2ï¸âƒ£ Káº¿t ná»‘i Kafka
```python
kafka_stream_df = spark.readStream \
    .format("kafka") \
    .option("subscribe", "user-events") \
    .option("startingOffsets", "latest") \
    .load()
```

#### 3ï¸âƒ£ Äá»‹nh nghÄ©a Schema
```python
user_events_schema = StructType([
    StructField("user_id", IntegerType(), True),
    StructField("movie_id", IntegerType(), True),
    StructField("event_type", StringType(), True),
    StructField("timestamp", LongType(), True),
])
```

#### 4ï¸âƒ£ Parse JSON
```python
parsed_events_df = kafka_stream_df \
    .selectExpr("CAST(value AS STRING)") \
    .withColumn("data", from_json(col("value"), user_events_schema)) \
    .select("data.*")
```

#### 5ï¸âƒ£ Transform Data
```python
transformed_events_df = parsed_events_df \
    .withColumn("event_timestamp", col("timestamp").cast(TimestampType())) \
    .withColumn("processed_at", current_timestamp())
```

#### 6ï¸âƒ£ Write to PostgreSQL
```python
def write_user_events_to_postgres(batch_df, epoch_id):
    batch_df.write.format("jdbc") \
        .option("dbtable", "user_events") \
        .mode("append") \
        .save()
```

#### 7ï¸âƒ£ Start Streaming
```python
query = transformed_events_df.writeStream \
    .outputMode("append") \
    .foreachBatch(write_user_events_to_postgres) \
    .option("checkpointLocation", "/tmp/checkpoint/user-events") \
    .start()
```

#### 8ï¸âƒ£ Await Termination
```python
query.awaitTermination()
```

---

## ğŸ—„ï¸ Database Schema

### Table: `user_events`
```sql
CREATE TABLE user_events (
    id SERIAL PRIMARY KEY,
    user_id INTEGER NOT NULL,
    movie_id INTEGER NOT NULL,
    event_type VARCHAR(50) NOT NULL,
    event_timestamp TIMESTAMP NOT NULL,
    processed_at TIMESTAMP NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

### Indexes
- `idx_user_events_user_id` - Query by user
- `idx_user_events_movie_id` - Query by movie
- `idx_user_events_event_type` - Filter by event type
- `idx_user_events_timestamp` - Query by time
- Composite indexes for common queries

### Views
- `user_events_stats` - Statistics by event type

### Functions
- `get_recent_events(limit)` - Get recent events
- `get_user_events(user_id, limit)` - Get user's events

---

## ğŸ“Š Data Flow

```
Web Client â†’ API Gateway â†’ Kafka (user-events) â†’ Spark Streaming â†’ PostgreSQL
```

### Input (Kafka Message)
```json
{
  "user_id": 123,
  "movie_id": 456,
  "event_type": "click",
  "timestamp": 1701234567890
}
```

### Output (PostgreSQL Record)
```
id | user_id | movie_id | event_type | event_timestamp      | processed_at         | created_at
---+---------+----------+------------+---------------------+----------------------+----------------------
1  | 123     | 456      | click      | 2024-11-30 15:30:00 | 2024-11-30 15:30:05 | 2024-11-30 15:30:05
```

---

## ğŸ” Monitoring

### Xem Events Má»›i Nháº¥t
```sql
SELECT * FROM user_events ORDER BY processed_at DESC LIMIT 10;
```

### Thá»‘ng KÃª
```sql
SELECT * FROM user_events_stats;
```

### Processing Rate
```sql
SELECT 
    DATE_TRUNC('minute', processed_at) as minute,
    COUNT(*) as events_processed
FROM user_events
WHERE processed_at > NOW() - INTERVAL '1 hour'
GROUP BY minute
ORDER BY minute DESC;
```

---

## ğŸ› ï¸ Troubleshooting

### Kafka Connection Error
```bash
# Kiá»ƒm tra Kafka Ä‘ang cháº¡y
docker ps | grep kafka

# Kiá»ƒm tra topic
kafka-topics --list --bootstrap-server localhost:9092
```

### PostgreSQL Connection Error
```bash
# Kiá»ƒm tra PostgreSQL Ä‘ang cháº¡y
docker ps | grep postgres

# Test connection
psql -U postgres -h localhost -d your_database
```

### Table Not Found Error
```sql
-- Táº¡o láº¡i báº£ng
\i setup_user_events_table.sql
```

---

## âš™ï¸ Configuration

### Environment Variables
| Variable | Description | Example |
|----------|-------------|---------|
| `KAFKA_BROKER` | Kafka server address | `localhost:9092` |
| `POSTGRES_USER` | Database username | `postgres` |
| `POSTGRES_PASSWORD` | Database password | `your_password` |
| `POSTGRES_HOST` | Database host | `localhost` |
| `POSTGRES_DB` | Database name | `your_database` |

### Spark Configuration
```python
# Thay Ä‘á»•i checkpoint location
.option("checkpointLocation", "/custom/path")

# Thay Ä‘á»•i starting offset
.option("startingOffsets", "earliest")  # Äá»c tá»« Ä‘áº§u
.option("startingOffsets", "latest")    # Chá»‰ Ä‘á»c má»›i
```

---

## ğŸ“ Kiáº¿n Thá»©c Cáº§n Thiáº¿t

### Technologies
- **Apache Spark**: Distributed processing framework
- **Kafka**: Distributed streaming platform
- **PostgreSQL**: Relational database
- **PySpark**: Python API for Spark

### Concepts
- **Structured Streaming**: Spark's streaming API
- **Micro-batching**: Processing data in small batches
- **Checkpoint**: Saving state for fault tolerance
- **foreachBatch**: Processing each batch with custom logic
- **JDBC**: Java Database Connectivity

---

## ğŸ“ˆ Performance Tips

1. **Kafka Partitions**: TÄƒng sá»‘ partitions Ä‘á»ƒ scale horizontal
2. **Batch Interval**: Äiá»u chá»‰nh processing time phÃ¹ há»£p
3. **Database Indexes**: ÄÃ£ táº¡o sáºµn indexes cho performance
4. **Checkpoint Cleanup**: XÃ³a checkpoint cÅ© khi thay Ä‘á»•i schema
5. **Monitoring**: Theo dÃµi lag vÃ  processing time

---

## ğŸ”’ Security

### Best Practices
- âœ… KhÃ´ng commit password vÃ o git
- âœ… Sá»­ dá»¥ng environment variables
- âœ… Encrypt database connections (SSL)
- âœ… Limit database user permissions
- âœ… Monitor access logs

---

## ğŸ§ª Testing

### Test vá»›i Sample Data
```bash
# Produce test message to Kafka
echo '{"user_id":123,"movie_id":456,"event_type":"click","timestamp":1701234567890}' | \
  kafka-console-producer --topic user-events --bootstrap-server localhost:9092
```

### Verify in Database
```sql
SELECT * FROM user_events WHERE user_id = 123;
```

---

## ğŸ³ Docker Support

### Build Image
```bash
docker build -t spark-processor .
```

### Run Container
```bash
docker run -e KAFKA_BROKER=kafka:9092 \
           -e POSTGRES_HOST=postgres \
           -e POSTGRES_USER=postgres \
           -e POSTGRES_PASSWORD=password \
           -e POSTGRES_DB=mydb \
           spark-processor
```

---

## ğŸ“ Support

### TÃ i Liá»‡u
- [README_USER_EVENTS.md](README_USER_EVENTS.md) - HÆ°á»›ng dáº«n chi tiáº¿t
- [ARCHITECTURE.md](ARCHITECTURE.md) - Kiáº¿n trÃºc há»‡ thá»‘ng
- [QUICK_REFERENCE.md](QUICK_REFERENCE.md) - Quick reference

### Common Issues
- Xem pháº§n Troubleshooting trong [README_USER_EVENTS.md](README_USER_EVENTS.md)
- Xem logs trong console output
- Check Spark UI táº¡i `http://localhost:4040`

---

## ğŸ“ License

This project is for educational purposes.

---

## ğŸ‘¥ Contributors

- Developed for Big Data course project
- Detailed Vietnamese comments for learning purposes

---

## ğŸ‰ Káº¿t Luáº­n

Há»‡ thá»‘ng nÃ y cung cáº¥p má»™t giáº£i phÃ¡p hoÃ n chá»‰nh cho viá»‡c xá»­ lÃ½ streaming data real-time tá»« Kafka vÃ o PostgreSQL sá»­ dá»¥ng Spark Structured Streaming. Code Ä‘Æ°á»£c viáº¿t vá»›i comment chi tiáº¿t báº±ng tiáº¿ng Viá»‡t Ä‘á»ƒ dá»… hiá»ƒu vÃ  há»c táº­p.

**Happy Streaming! ğŸš€**
