# ğŸ”§ Troubleshooting: Spark Worker Connection Issue

## âŒ Váº¥n Äá»

Logs hiá»ƒn thá»‹ cáº£nh bÃ¡o:
```
WARN TaskSchedulerImpl: Initial job has not accepted any resources; 
check your cluster UI to ensure that workers are registered and have sufficient resources
```

## ğŸ” NguyÃªn NhÃ¢n

CÃ³ 3 nguyÃªn nhÃ¢n chÃ­nh:

### 1. Spark Worker ChÆ°a ÄÄƒng KÃ½ vá»›i Master
- Worker container cháº¡y nhÆ°ng khÃ´ng káº¿t ná»‘i Ä‘Æ°á»£c vá»›i Master
- Network issues
- Master URL khÃ´ng Ä‘Ãºng

### 2. Worker KhÃ´ng Äá»§ TÃ i NguyÃªn
- Memory/CPU Ä‘Æ°á»£c cáº¥u hÃ¬nh quÃ¡ cao
- Host machine khÃ´ng Ä‘á»§ resources

### 3. Timing Issue
- Processor start quÃ¡ sá»›m, trÆ°á»›c khi Worker ready
- Sleep 40s cÃ³ thá»ƒ chÆ°a Ä‘á»§

---

## âœ… CÃ¡ch Kiá»ƒm Tra

### BÆ°á»›c 1: Kiá»ƒm Tra Status Containers

```powershell
# Xem táº¥t cáº£ containers
docker ps -a

# Hoáº·c vá»›i Docker Compose
docker compose ps
```

**Kiá»ƒm tra:**
- `spark-master` - Status pháº£i lÃ  `Up`
- `spark-worker` - Status pháº£i lÃ  `Up`
- `spark-processor-*` - CÃ³ thá»ƒ `Restarting` náº¿u lá»—i

### BÆ°á»›c 2: Xem Logs Spark Master

```powershell
docker logs spark-master --tail=100
```

**TÃ¬m kiáº¿m:**
- `Registering worker` - Worker Ä‘Ã£ Ä‘Äƒng kÃ½ thÃ nh cÃ´ng
- Náº¿u KHÃ”NG tháº¥y â†’ Worker chÆ°a káº¿t ná»‘i

### BÆ°á»›c 3: Xem Logs Spark Worker

```powershell
docker logs spark-worker --tail=100
```

**TÃ¬m kiáº¿m:**
- `Successfully registered with master` - Káº¿t ná»‘i OK
- `Failed to connect` hoáº·c `Connection refused` - Lá»—i káº¿t ná»‘i

### BÆ°á»›c 4: Kiá»ƒm Tra Spark UI

Má»Ÿ browser vÃ  truy cáº­p:
```
http://localhost:8082
```

**Kiá»ƒm tra:**
- **Workers** tab: Pháº£i cÃ³ Ã­t nháº¥t 1 worker
- **Cores**: Sá»‘ cores available > 0
- **Memory**: Memory available > 0

Náº¿u khÃ´ng cÃ³ worker nÃ o â†’ Worker chÆ°a Ä‘Äƒng kÃ½!

---

## ğŸ”§ Giáº£i PhÃ¡p

### Giáº£i PhÃ¡p 1: TÄƒng Sleep Time

Náº¿u Worker cáº§n nhiá»u thá»i gian hÆ¡n Ä‘á»ƒ start:

**File**: `docker-compose.yml`

```yaml
spark-processor-user-events:
  command: >
    /bin/bash -c "
    echo 'Waiting for Spark cluster to be ready...';
    sleep 60;  # â† TÄƒng tá»« 40 lÃªn 60 giÃ¢y
    ...
```

### Giáº£i PhÃ¡p 2: Giáº£m Resource Requirements

Náº¿u Worker khÃ´ng Ä‘á»§ resources:

**File**: `docker-compose.yml`

```yaml
spark-worker:
  environment:
    - SPARK_WORKER_CORES=1      # â† Giáº£m tá»« 2 xuá»‘ng 1
    - SPARK_WORKER_MEMORY=1G    # â† Giáº£m tá»« 2G xuá»‘ng 1G
```

VÃ  trong processor command:

```yaml
spark-processor-user-events:
  command: >
    /bin/bash -c "
    ...
    /opt/spark/bin/spark-submit 
      --driver-memory 512m      # â† Giáº£m tá»« 1g xuá»‘ng 512m
      --executor-memory 512m    # â† Giáº£m tá»« 1g xuá»‘ng 512m
      --executor-cores 1 
      ...
```

### Giáº£i PhÃ¡p 3: Sá»­ Dá»¥ng Local Mode (KhÃ´ng Cáº§n Worker)

Náº¿u váº«n khÃ´ng Ä‘Æ°á»£c, cháº¡y Spark á»Ÿ local mode:

**File**: `docker-compose.yml`

```yaml
spark-processor-user-events:
  command: >
    /bin/bash -c "
    echo 'Waiting for dependencies...';
    sleep 20;
    sed -i 's/\r$//' /app/processor.py;
    /opt/spark/bin/spark-submit 
      --master local[2]  # â† Thay vÃ¬ spark://spark-master:7077
      --driver-memory 1g 
      --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.postgresql:postgresql:42.6.0 
      --conf spark.sql.streaming.checkpointLocation=/tmp/checkpoint/user-events 
      /app/processor.py
    "
```

**LÆ°u Ã½**: Local mode khÃ´ng cáº§n spark-master vÃ  spark-worker!

### Giáº£i PhÃ¡p 4: ThÃªm Health Check Dependency

Äáº£m báº£o processor chá»‰ start khi worker Ä‘Ã£ ready:

**File**: `docker-compose.yml`

ThÃªm healthcheck cho spark-worker:

```yaml
spark-worker:
  # ... existing config ...
  healthcheck:
    test: ["CMD", "curl", "-f", "http://localhost:8081"]
    interval: 10s
    timeout: 5s
    retries: 5
    start_period: 30s
```

VÃ  update dependency:

```yaml
spark-processor-user-events:
  depends_on:
    spark-worker:
      condition: service_healthy  # â† Thay vÃ¬ service_started
```

---

## ğŸ“‹ Checklist Troubleshooting

HÃ£y lÃ m theo thá»© tá»±:

- [ ] 1. Kiá»ƒm tra `docker ps` - Táº¥t cáº£ containers Ä‘ang cháº¡y?
- [ ] 2. Xem `docker logs spark-master` - Worker Ä‘Ã£ register?
- [ ] 3. Xem `docker logs spark-worker` - CÃ³ lá»—i connection?
- [ ] 4. Má»Ÿ `http://localhost:8082` - CÃ³ worker trong UI?
- [ ] 5. Náº¿u khÃ´ng cÃ³ worker â†’ Restart worker: `docker restart spark-worker`
- [ ] 6. Náº¿u váº«n lá»—i â†’ Ãp dá»¥ng Giáº£i phÃ¡p 1 (tÄƒng sleep time)
- [ ] 7. Náº¿u váº«n lá»—i â†’ Ãp dá»¥ng Giáº£i phÃ¡p 2 (giáº£m resources)
- [ ] 8. Náº¿u váº«n lá»—i â†’ Ãp dá»¥ng Giáº£i phÃ¡p 3 (local mode)

---

## ğŸš€ Quick Fix (Recommended)

**CÃ¡ch nhanh nháº¥t**: Sá»­ dá»¥ng Local Mode

1. **Sá»­a docker-compose.yml**:

```yaml
spark-processor-user-events:
  build: ./spark_processor
  container_name: spark-processor-user-events
  env_file: .env
  environment:
    - RUN_MODE=user_events
  networks:
    - app-network
  depends_on:
    kafka:
      condition: service_healthy
    postgres-db:
      condition: service_healthy
  restart: on-failure
  command: >
    /bin/bash -c "
    echo 'Starting User Events Processor...';
    sleep 20;
    sed -i 's/\r$//' /app/processor.py;
    /opt/spark/bin/spark-submit --master local[2] --driver-memory 1g --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.postgresql:postgresql:42.6.0 --conf spark.sql.streaming.checkpointLocation=/tmp/checkpoint/user-events /app/processor.py
    "
```

2. **Restart**:

```powershell
docker compose down spark-processor-user-events
docker compose up -d spark-processor-user-events
docker compose logs -f spark-processor-user-events
```

---

## ğŸ“Š So SÃ¡nh Modes

| Feature | Cluster Mode | Local Mode |
|---------|--------------|------------|
| **Master** | spark://spark-master:7077 | local[2] |
| **Workers** | Cáº§n spark-worker | KhÃ´ng cáº§n |
| **Scalability** | CÃ³ thá»ƒ scale | KhÃ´ng scale Ä‘Æ°á»£c |
| **Setup** | Phá»©c táº¡p hÆ¡n | ÄÆ¡n giáº£n |
| **Resources** | Cáº§n nhiá»u RAM | Ãt RAM hÆ¡n |
| **Use Case** | Production, big data | Development, testing |

**Khuyáº¿n nghá»‹**: 
- Development/Testing â†’ DÃ¹ng **Local Mode**
- Production â†’ DÃ¹ng **Cluster Mode** (cáº§n fix worker issue)

---

## ğŸ’¡ Tips

1. **Kiá»ƒm tra RAM**: Äáº£m báº£o mÃ¡y cÃ³ Ã­t nháº¥t 4GB RAM free
2. **Docker Resources**: Trong Docker Desktop â†’ Settings â†’ Resources â†’ TÄƒng Memory lÃªn 4GB+
3. **Logs**: LuÃ´n check logs cá»§a cáº£ 3: master, worker, processor
4. **UI**: Spark UI ráº¥t há»¯u Ã­ch Ä‘á»ƒ debug

---

## ğŸ¯ Next Steps

Sau khi fix:

```powershell
# 1. Restart services
docker compose restart spark-processor-user-events

# 2. Xem logs
docker compose logs -f spark-processor-user-events

# 3. Test vá»›i message
docker exec -it kafka kafka-console-producer --topic user-events --bootstrap-server localhost:9092
# Input: {"user_id":123,"movie_id":456,"event_type":"click","timestamp":1701234567890}

# 4. Kiá»ƒm tra database
docker exec -it postgres-db psql -U postgres -d your_db -c "SELECT * FROM user_events LIMIT 5;"
```

---

**HÃ£y thá»­ cÃ¡c bÆ°á»›c trÃªn vÃ  cho tÃ´i biáº¿t káº¿t quáº£!** ğŸš€
