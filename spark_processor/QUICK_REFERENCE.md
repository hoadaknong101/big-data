# Quick Reference - User Events Subscriber

## ðŸš€ Cháº¡y Nhanh (Quick Start)

### 1. Setup Database
```bash
psql -U postgres -d your_db -f setup_user_events_table.sql
```

### 2. Set Environment
```bash
export KAFKA_BROKER=localhost:9092
export POSTGRES_USER=postgres
export POSTGRES_PASSWORD=your_password
export POSTGRES_HOST=localhost
export POSTGRES_DB=your_database
```

### 3. Run Subscriber
```bash
python run_user_events_subscriber.py
```

---

## ðŸ“ Code Snippets

### Import vÃ  Gá»i HÃ m
```python
from processor import subscribe_user_events

# Cháº¡y subscriber
subscribe_user_events()
```

### Kiá»ƒm Tra Environment
```python
import os
print(f"Kafka: {os.environ.get('KAFKA_BROKER')}")
print(f"DB: {os.environ.get('POSTGRES_HOST')}")
```

---

## ðŸ—„ï¸ SQL Queries

### Xem Táº¥t Cáº£ Events
```sql
SELECT * FROM user_events ORDER BY processed_at DESC LIMIT 10;
```

### Äáº¿m Events
```sql
SELECT COUNT(*) FROM user_events;
```

### Thá»‘ng KÃª Theo Event Type
```sql
SELECT * FROM user_events_stats;
```

### Events Cá»§a User
```sql
SELECT * FROM get_user_events(123, 50);
```

### Events Má»›i Nháº¥t
```sql
SELECT * FROM get_recent_events(10);
```

### Events Trong 1 Giá» Qua
```sql
SELECT * FROM user_events 
WHERE event_timestamp > NOW() - INTERVAL '1 hour'
ORDER BY event_timestamp DESC;
```

### Top Users
```sql
SELECT user_id, COUNT(*) as event_count
FROM user_events
GROUP BY user_id
ORDER BY event_count DESC
LIMIT 10;
```

### Top Movies
```sql
SELECT movie_id, COUNT(*) as interaction_count
FROM user_events
GROUP BY movie_id
ORDER BY interaction_count DESC
LIMIT 10;
```

---

## ðŸ” Debugging Commands

### Kiá»ƒm Tra Kafka
```bash
# List topics
kafka-topics --list --bootstrap-server localhost:9092

# Describe topic
kafka-topics --describe --topic user-events --bootstrap-server localhost:9092

# Consume messages
kafka-console-consumer --topic user-events --from-beginning --bootstrap-server localhost:9092
```

### Kiá»ƒm Tra PostgreSQL
```bash
# Connect
psql -U postgres -d your_db

# List tables
\dt

# Describe table
\d user_events

# Check connections
SELECT * FROM pg_stat_activity WHERE datname = 'your_db';
```

### Kiá»ƒm Tra Spark
```bash
# Check if running
ps aux | grep spark

# View logs
tail -f /path/to/spark/logs/spark-*.log
```

---

## ðŸ› ï¸ Troubleshooting

### Lá»—i: Connection refused (Kafka)
```bash
# Kiá»ƒm tra Kafka Ä‘ang cháº¡y
docker ps | grep kafka

# Restart Kafka
docker restart kafka
```

### Lá»—i: Connection refused (PostgreSQL)
```bash
# Kiá»ƒm tra PostgreSQL Ä‘ang cháº¡y
docker ps | grep postgres

# Restart PostgreSQL
docker restart postgres
```

### Lá»—i: Table does not exist
```sql
-- Táº¡o láº¡i báº£ng
\i setup_user_events_table.sql
```

### Lá»—i: Permission denied
```sql
-- Grant permissions
GRANT ALL PRIVILEGES ON TABLE user_events TO your_user;
GRANT USAGE, SELECT ON SEQUENCE user_events_id_seq TO your_user;
```

---

## ðŸ“Š Monitoring

### Xem Processing Rate
```sql
SELECT 
    DATE_TRUNC('minute', processed_at) as minute,
    COUNT(*) as events_processed
FROM user_events
WHERE processed_at > NOW() - INTERVAL '1 hour'
GROUP BY minute
ORDER BY minute DESC;
```

### Xem Latency
```sql
SELECT 
    AVG(EXTRACT(EPOCH FROM (processed_at - event_timestamp))) as avg_latency_seconds
FROM user_events
WHERE processed_at > NOW() - INTERVAL '1 hour';
```

### Xem Table Size
```sql
SELECT 
    pg_size_pretty(pg_total_relation_size('user_events')) as total_size,
    pg_size_pretty(pg_relation_size('user_events')) as table_size,
    pg_size_pretty(pg_indexes_size('user_events')) as indexes_size;
```

---

## ðŸŽ¯ Common Tasks

### XÃ³a Dá»¯ Liá»‡u CÅ©
```sql
-- XÃ³a events cÅ© hÆ¡n 30 ngÃ y
DELETE FROM user_events 
WHERE event_timestamp < NOW() - INTERVAL '30 days';
```

### Backup Table
```bash
pg_dump -U postgres -d your_db -t user_events > user_events_backup.sql
```

### Restore Table
```bash
psql -U postgres -d your_db < user_events_backup.sql
```

### Export to CSV
```sql
COPY (SELECT * FROM user_events) TO '/tmp/user_events.csv' CSV HEADER;
```

---

## ðŸ”§ Configuration

### Thay Äá»•i Checkpoint Location
```python
.option("checkpointLocation", "/your/custom/path")
```

### Thay Äá»•i Starting Offset
```python
.option("startingOffsets", "earliest")  # Äá»c tá»« Ä‘áº§u
.option("startingOffsets", "latest")    # Chá»‰ Ä‘á»c má»›i
```

### Thay Äá»•i Batch Interval
```python
spark.conf.set("spark.sql.streaming.trigger.processingTime", "10 seconds")
```

---

## ðŸ“ž Quick Help

### Files
- `processor.py` - Code chÃ­nh
- `run_user_events_subscriber.py` - Script cháº¡y
- `setup_user_events_table.sql` - Database setup
- `README_USER_EVENTS.md` - Documentation Ä‘áº§y Ä‘á»§
- `ARCHITECTURE.md` - SÆ¡ Ä‘á»“ kiáº¿n trÃºc
- `SUMMARY.md` - TÃ³m táº¯t cÃ´ng viá»‡c

### Environment Variables
- `KAFKA_BROKER` - Kafka server address
- `POSTGRES_USER` - Database user
- `POSTGRES_PASSWORD` - Database password
- `POSTGRES_HOST` - Database host
- `POSTGRES_DB` - Database name

### Key Concepts
- **Streaming**: Xá»­ lÃ½ dá»¯ liá»‡u liÃªn tá»¥c
- **Micro-batch**: Xá»­ lÃ½ theo batch nhá»
- **Checkpoint**: LÆ°u tráº¡ng thÃ¡i Ä‘á»ƒ recovery
- **foreachBatch**: Xá»­ lÃ½ tá»«ng batch
- **Fault Tolerance**: Kháº£ nÄƒng chá»‹u lá»—i

---

## ðŸ’¡ Tips

1. **Performance**: TÄƒng sá»‘ partitions trong Kafka Ä‘á»ƒ scale
2. **Monitoring**: Theo dÃµi lag vÃ  processing time
3. **Cleanup**: XÃ³a checkpoint cÅ© khi thay Ä‘á»•i schema
4. **Testing**: Test vá»›i Ã­t data trÆ°á»›c khi production
5. **Backup**: Backup checkpoint vÃ  database thÆ°á»ng xuyÃªn

---

**Cáº§n thÃªm trá»£ giÃºp? Xem `README_USER_EVENTS.md`**
