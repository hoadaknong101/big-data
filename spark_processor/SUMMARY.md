# TÃ³m Táº¯t CÃ´ng Viá»‡c HoÃ n ThÃ nh

## ğŸ“‹ YÃªu Cáº§u
XÃ¢y dá»±ng hÃ m thá»±c hiá»‡n subscribe topic `user-events` tá»« Kafka, lÆ°u message má»›i vÃ o database real-time vá»›i comment chi tiáº¿t.

## âœ… ÄÃ£ HoÃ n ThÃ nh

### 1. File `processor.py` - HÃ m ChÃ­nh
**ÄÃ£ thÃªm hÃ m `subscribe_user_events()`** vá»›i cÃ¡c tÃ­nh nÄƒng:

#### ğŸ”¹ BÆ°á»›c 1: Khá»Ÿi táº¡o Spark Session
```python
spark = SparkSession.builder.appName("UserEventsSubscriber").getOrCreate()
```
- Táº¡o Spark Session Ä‘á»ƒ xá»­ lÃ½ streaming data
- Set log level = WARN Ä‘á»ƒ giáº£m nhiá»…u

#### ğŸ”¹ BÆ°á»›c 2: Káº¿t ná»‘i Kafka vÃ  Subscribe Topic
```python
kafka_stream_df = spark.readStream \
    .format("kafka") \
    .option("subscribe", "user-events") \
    .option("startingOffsets", "latest") \
    .load()
```
- Subscribe topic `user-events` 
- Chá»‰ Ä‘á»c message má»›i (latest)
- Táº¡o streaming DataFrame

#### ğŸ”¹ BÆ°á»›c 3: Äá»‹nh nghÄ©a Schema
```python
user_events_schema = StructType([
    StructField("user_id", IntegerType(), True),
    StructField("movie_id", IntegerType(), True),
    StructField("event_type", StringType(), True),
    StructField("timestamp", LongType(), True),
])
```
- Schema cho JSON message
- TÄƒng hiá»‡u suáº¥t parsing

#### ğŸ”¹ BÆ°á»›c 4: Parse JSON tá»« Kafka Message
```python
parsed_events_df = kafka_stream_df \
    .selectExpr("CAST(value AS STRING)") \
    .withColumn("data", from_json(col("value"), user_events_schema)) \
    .select("data.*")
```
- Chuyá»ƒn binary â†’ string
- Parse JSON theo schema
- Extract fields

#### ğŸ”¹ BÆ°á»›c 5: Transform Data
```python
transformed_events_df = parsed_events_df \
    .withColumn("event_timestamp", col("timestamp").cast(TimestampType())) \
    .withColumn("processed_at", current_timestamp()) \
    .select("user_id", "movie_id", "event_type", "event_timestamp", "processed_at")
```
- Convert Unix timestamp â†’ TimestampType
- ThÃªm cá»™t `processed_at`

#### ğŸ”¹ BÆ°á»›c 6: HÃ m Ghi Database
```python
def write_user_events_to_postgres(batch_df, epoch_id):
    batch_df.write.format("jdbc") \
        .option("dbtable", "user_events") \
        .mode("append") \
        .save()
```
- Ghi tá»«ng micro-batch vÃ o PostgreSQL
- Mode append (khÃ´ng ghi Ä‘Ã¨)
- Hiá»ƒn thá»‹ sample data

#### ğŸ”¹ BÆ°á»›c 7: Báº¯t Ä‘áº§u Streaming
```python
query = transformed_events_df.writeStream \
    .outputMode("append") \
    .foreachBatch(write_user_events_to_postgres) \
    .option("checkpointLocation", "/tmp/checkpoint/user-events") \
    .start()
```
- Start streaming query
- Checkpoint Ä‘á»ƒ recovery
- foreachBatch processing

#### ğŸ”¹ BÆ°á»›c 8: Cháº¡y LiÃªn Tá»¥c
```python
query.awaitTermination()
```
- Cháº¡y 24/7
- Äá»£i message má»›i
- Xá»­ lÃ½ exception (Ctrl+C, errors)

### 2. File `setup_user_events_table.sql` - Database Schema
**Script SQL hoÃ n chá»‰nh** bao gá»“m:

```sql
CREATE TABLE user_events (
    id SERIAL PRIMARY KEY,
    user_id INTEGER NOT NULL,
    movie_id INTEGER NOT NULL,
    event_type VARCHAR(50) NOT NULL,
    event_timestamp TIMESTAMP NOT NULL,
    processed_at TIMESTAMP NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

**Indexes** Ä‘á»ƒ tÄƒng tá»‘c:
- `idx_user_events_user_id`
- `idx_user_events_movie_id`
- `idx_user_events_event_type`
- `idx_user_events_timestamp`
- Composite indexes

**Helper Functions**:
- `get_recent_events(limit)` - Xem events má»›i nháº¥t
- `get_user_events(user_id, limit)` - Xem events cá»§a user

**View**:
- `user_events_stats` - Thá»‘ng kÃª nhanh

### 3. File `run_user_events_subscriber.py` - Script Cháº¡y
**Standalone script** vá»›i:
- âœ… Kiá»ƒm tra environment variables
- âœ… Error handling
- âœ… HÆ°á»›ng dáº«n kháº¯c phá»¥c lá»—i
- âœ… Graceful shutdown (Ctrl+C)

### 4. File `README_USER_EVENTS.md` - TÃ i Liá»‡u
**Documentation Ä‘áº§y Ä‘á»§** bao gá»“m:
- ğŸ“– Giáº£i thÃ­ch chi tiáº¿t tá»«ng bÆ°á»›c
- ğŸš€ HÆ°á»›ng dáº«n sá»­ dá»¥ng (3 options)
- ğŸ” Monitoring & debugging
- ğŸ› ï¸ Troubleshooting
- ğŸ“Š Performance tips
- ğŸ“ Sample queries

## ğŸ¯ TÃ­nh NÄƒng ChÃ­nh

### âœ¨ Real-time Processing
- Cháº¡y liÃªn tá»¥c 24/7
- Tá»± Ä‘á»™ng xá»­ lÃ½ message má»›i
- Latency tháº¥p (vÃ i giÃ¢y)

### ğŸ”„ Fault Tolerance
- Checkpoint mechanism
- Auto recovery khi lá»—i
- KhÃ´ng máº¥t dá»¯ liá»‡u

### ğŸ“Š Monitoring
- Log chi tiáº¿t tá»«ng bÆ°á»›c
- Hiá»ƒn thá»‹ sample data
- Thá»‘ng kÃª real-time

### ğŸ’¾ Data Storage
- LÆ°u Táº¤T Cáº¢ events (khÃ´ng filter)
- Append mode (khÃ´ng ghi Ä‘Ã¨)
- Index tá»‘i Æ°u cho query

## ğŸ“ Cáº¥u TrÃºc Files

```
spark_processor/
â”œâ”€â”€ processor.py                      # âœ… Code chÃ­nh (Ä‘Ã£ update)
â”œâ”€â”€ run_user_events_subscriber.py     # âœ… Script cháº¡y
â”œâ”€â”€ setup_user_events_table.sql       # âœ… Database setup
â”œâ”€â”€ README_USER_EVENTS.md             # âœ… Documentation
â”œâ”€â”€ requirements.txt                  # Existing
â””â”€â”€ Dockerfile                        # Existing
```

## ğŸš€ CÃ¡ch Sá»­ Dá»¥ng Nhanh

### BÆ°á»›c 1: Setup Database
```bash
psql -U your_user -d your_database -f setup_user_events_table.sql
```

### BÆ°á»›c 2: Set Environment Variables
```bash
export KAFKA_BROKER=localhost:9092
export POSTGRES_USER=your_user
export POSTGRES_PASSWORD=your_password
export POSTGRES_HOST=localhost
export POSTGRES_DB=your_database
```

### BÆ°á»›c 3: Cháº¡y Subscriber
**Option A**: DÃ¹ng script standalone
```bash
python run_user_events_subscriber.py
```

**Option B**: Sá»­a processor.py
```python
if __name__ == "__main__":
    subscribe_user_events()  # Uncomment dÃ²ng nÃ y
    # main()  # Comment dÃ²ng nÃ y
```
Rá»“i cháº¡y:
```bash
python processor.py
```

## ğŸ“Š Data Flow

```
Kafka Topic (user-events)
    â†“
Spark Streaming (subscribe_user_events)
    â†“
Parse JSON + Transform
    â†“
PostgreSQL (user_events table)
```

## ğŸ” Monitoring Queries

```sql
-- Tá»•ng sá»‘ events
SELECT COUNT(*) FROM user_events;

-- Events theo loáº¡i
SELECT * FROM user_events_stats;

-- 10 events má»›i nháº¥t
SELECT * FROM get_recent_events(10);

-- Events cá»§a user 123
SELECT * FROM get_user_events(123);
```

## ğŸ’¡ Äiá»ƒm Ná»•i Báº­t

### 1. Comment Chi Tiáº¿t
âœ… Má»—i bÆ°á»›c cÃ³ comment giáº£i thÃ­ch báº±ng tiáº¿ng Viá»‡t
âœ… Giáº£i thÃ­ch táº¡i sao (why), khÃ´ng chá»‰ lÃ  gÃ¬ (what)
âœ… Code dá»… hiá»ƒu cho ngÆ°á»i má»›i

### 2. Production-Ready
âœ… Error handling Ä‘áº§y Ä‘á»§
âœ… Checkpoint cho fault tolerance
âœ… Logging chi tiáº¿t
âœ… Performance optimization (indexes)

### 3. Developer-Friendly
âœ… Documentation Ä‘áº§y Ä‘á»§
âœ… Helper scripts
âœ… Sample queries
âœ… Troubleshooting guide

## ğŸ“ Kiáº¿n Thá»©c Ãp Dá»¥ng

### Spark Streaming
- âœ… readStream API
- âœ… Structured Streaming
- âœ… foreachBatch processing
- âœ… Checkpoint mechanism

### Kafka Integration
- âœ… Subscribe topic
- âœ… Parse Kafka message
- âœ… Offset management

### Data Processing
- âœ… JSON parsing vá»›i schema
- âœ… Data transformation
- âœ… Type casting

### Database
- âœ… JDBC connection
- âœ… Batch writing
- âœ… Index optimization

## ğŸ† Káº¿t Quáº£

âœ… **HÃ m `subscribe_user_events()` hoÃ n chá»‰nh**
- Subscribe topic `user-events` âœ“
- LÆ°u real-time vÃ o database âœ“
- Comment chi tiáº¿t tá»«ng bÆ°á»›c âœ“
- Cháº¡y liÃªn tá»¥c 24/7 âœ“

âœ… **Database schema Ä‘áº§y Ä‘á»§**
- Table vá»›i indexes âœ“
- Helper functions âœ“
- Stats view âœ“

âœ… **Documentation hoÃ n chá»‰nh**
- HÆ°á»›ng dáº«n sá»­ dá»¥ng âœ“
- Troubleshooting âœ“
- Sample queries âœ“

âœ… **Production-ready code**
- Error handling âœ“
- Fault tolerance âœ“
- Monitoring âœ“

---

**Táº¥t cáº£ yÃªu cáº§u Ä‘Ã£ Ä‘Æ°á»£c hoÃ n thÃ nh!** ğŸ‰
